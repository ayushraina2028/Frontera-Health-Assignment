{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 04:43:05.884096: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 04:43:05.891668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740352385.900535    8897 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740352385.903215    8897 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 04:43:05.912753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2VecClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Wav2VecClassifier, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained Wav2Vec model\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        \n",
    "        # Freeze the feature extractor layers (optional)\n",
    "        for param in self.wav2vec.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Get the output dimension of wav2vec\n",
    "        hidden_size = self.wav2vec.config.hidden_size  # typically 768\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get wav2vec features\n",
    "        outputs = self.wav2vec(x)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Pool the output (take mean of all time steps)\n",
    "        pooled_output = torch.mean(hidden_states, dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Let's test the model with random data\n",
    "def test_model():\n",
    "    # Create model instance\n",
    "    model = Wav2VecClassifier(num_classes=3)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create random input tensor\n",
    "    # Wav2Vec expects input shape: [batch_size, sequence_length]\n",
    "    # Typical audio sampling rate is 16kHz, let's create 1 second of audio\n",
    "    batch_size = 2\n",
    "    sequence_length = 16000  # 1 second of audio at 16kHz\n",
    "    random_audio = torch.randn(batch_size, sequence_length)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(random_audio)\n",
    "    \n",
    "    print(\"Input shape:\", random_audio.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output (logits):\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushraina/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Wav2VecClassifier(num_classes=3)\n",
    "model.load_state_dict(torch.load('wav2vec_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Test the model on test set in CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path):\n",
    "    # Load audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to numpy array and squeeze\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    \n",
    "    # Normalize audio\n",
    "    waveform = waveform / np.max(np.abs(waveform))\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def test_model_on_audio(model, audio_path, device):\n",
    "    class_mapping = {\n",
    "            'Cry': 0,\n",
    "            'NotScreaming': 1,\n",
    "            'Screaming': 2\n",
    "        }\n",
    "    \n",
    "    # Preprocess audio\n",
    "    waveform = preprocess_audio(audio_path)\n",
    "    input_values = torch.FloatTensor(waveform).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    \n",
    "    # Get predicted class\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Get class name\n",
    "    class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_class = class_mapping[predicted.item()]\n",
    "    \n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Cry\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"Datasets/Cry/0c8f14a9-6999-485b-97a2-913c1cbf099c-1430760379259-1.7-m-26-hu.wav\"\n",
    "predicted_class = test_model_on_audio(model, audio_path, device)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
