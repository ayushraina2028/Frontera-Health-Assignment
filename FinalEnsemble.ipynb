{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class mapping\n",
    "CLASS_MAPPING = {0: \"Cry\", 1: \"Screaming\", 2: \"NotScreaming\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Force TensorFlow to run on CPU (Prevents GPU memory usage)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Load YAMNet model\n",
    "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "def extract_embedding(file_path):\n",
    "    waveform, sr = librosa.load(file_path, sr=16000)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    scores, embeddings, spectrogram = yamnet_model(waveform)\n",
    "    tf.keras.backend.clear_session()  # ✅ Free memory\n",
    "    return np.mean(embeddings.numpy(), axis=0)\n",
    "\n",
    "def get_yamnet_prediction(model,filename):\n",
    "    embedding = extract_embedding(filename)\n",
    "    embedding = np.expand_dims(embedding, axis=0)\n",
    "    prediction = model.predict(embedding)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    print(f\"Predicted Class: {CLASS_MAPPING[predicted_class]}\")\n",
    "    print(f\"Prediction Scores: {prediction[0]}\")\n",
    "    \n",
    "    answer = CLASS_MAPPING[predicted_class]\n",
    "    probabilities = prediction[0]\n",
    "    return answer, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set GPU device to visible again\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2VecClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Wav2VecClassifier, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained Wav2Vec model\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        \n",
    "        # Freeze the feature extractor layers (optional)\n",
    "        for param in self.wav2vec.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Get the output dimension of wav2vec\n",
    "        hidden_size = self.wav2vec.config.hidden_size  # typically 768\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get wav2vec features\n",
    "        outputs = self.wav2vec(x)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Pool the output (take mean of all time steps)\n",
    "        pooled_output = torch.mean(hidden_states, dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Let's test the model with random data\n",
    "def test_model():\n",
    "    # Create model instance\n",
    "    model = Wav2VecClassifier(num_classes=3)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create random input tensor\n",
    "    # Wav2Vec expects input shape: [batch_size, sequence_length]\n",
    "    # Typical audio sampling rate is 16kHz, let's create 1 second of audio\n",
    "    batch_size = 2\n",
    "    sequence_length = 16000  # 1 second of audio at 16kHz\n",
    "    random_audio = torch.randn(batch_size, sequence_length)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(random_audio)\n",
    "    \n",
    "    print(\"Input shape:\", random_audio.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output (logits):\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path):\n",
    "    # Load audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to numpy array and squeeze\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    \n",
    "    # Normalize audio\n",
    "    waveform = waveform / np.max(np.abs(waveform))\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def test_model_on_audio(model, audio_path, device):\n",
    "    class_mapping = {\n",
    "            'Cry': 0,\n",
    "            'NotScreaming': 1,\n",
    "            'Screaming': 2\n",
    "        }\n",
    "    \n",
    "    # Preprocess audio\n",
    "    waveform = preprocess_audio(audio_path)\n",
    "    input_values = torch.FloatTensor(waveform).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    \n",
    "    # Get predicted class\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Get class name\n",
    "    class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_class = class_mapping[predicted.item()]\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=0).cpu().numpy()\n",
    "    return predicted_class, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x766f842c5e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x766f842c5e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Class: Cry\n",
      "Prediction Scores: [9.9248135e-01 7.2007808e-03 3.1792070e-04]\n"
     ]
    }
   ],
   "source": [
    "# Load Yamnet model\n",
    "model_yamnet = tf.keras.models.load_model(\"yamnet_finetuned.h5\")\n",
    "\n",
    "# Load Wav2Vec model\n",
    "model_wav2vec = Wav2VecClassifier(num_classes=3)\n",
    "model_wav2vec.load_state_dict(torch.load('wav2vec_model.pth'))\n",
    "model_wav2vec.eval()\n",
    "\n",
    "# Test the model on test set in CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_wav2vec = model_wav2vec.to(device)\n",
    "\n",
    "# Test our trained model on random audio file\n",
    "filename = \"Datasets/Cry/0c8f14a9-6999-485b-97a2-913c1cbf099c-1430760379259-1.7-m-26-hu.wav\"\n",
    "\n",
    "ans_yamnet, probabilities_yamnet = get_yamnet_prediction(model_yamnet,filename)\n",
    "ans_wav2vec, probabilities_wav2vec = test_model_on_audio(model_wav2vec, filename, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yamnet Prediction: Cry\n",
      "Wav2Vec Prediction: Cry\n"
     ]
    }
   ],
   "source": [
    "print(f\"Yamnet Prediction: {ans_yamnet}\")\n",
    "print(f\"Wav2Vec Prediction: {ans_wav2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yamnet Probabilities: [9.9248135e-01 7.2007808e-03 3.1792070e-04]\n",
      "Wav2Vec Probabilities: [9.981421e-01 7.241565e-04 1.133691e-03]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Yamnet Probabilities: {probabilities_yamnet}\")\n",
    "print(f\"Wav2Vec Probabilities: {probabilities_wav2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
